{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning TA application\n",
    "\n",
    "\n",
    "\n",
    "- Introduction\n",
    "  \n",
    "- Brief recap of the concept of AutoGrad and why it is useful\n",
    "  \n",
    "- Coding Exercise 2.1: Buiding a Computational Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PhD Candidate in NLP\n",
    "  \n",
    "- Certified Software Carpentry Instructor\n",
    "\n",
    "- Faculty member at Bayero University, Kano - Nigeria\n",
    "\n",
    "- Founder HausaNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Pytorch Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A machine learning model is a function, with inputs and outputs.\n",
    "\n",
    "- In training a model, we want to minimize the loss. \n",
    "\n",
    "- In the idealized case of a perfect model, that means adjusting its learning weights such that loss is zero for all inputs. But, in real world we can get tolerable loss for a wide variety of inputs.\n",
    "\n",
    "- How do we decide how far and in which direction to nudge the weights? \n",
    "\n",
    "- So, the gradients over the learning weights  tell us what direction to change each weight to get the loss function closer to zero.\n",
    "\n",
    "- Since the number of such local derivatives will tend to go up exponentially with the depth of a neural network, so does the complexity in computing them. \n",
    "\n",
    "- This is where autograd comes in: \n",
    "    1. It tracks the history of every computation. \n",
    "    2. Every computed tensor in your PyTorch model carries a history of its input tensors and the function used to create it. \n",
    "    3. Combined with the fact that PyTorch functions meant to act on tensors each have a built-in implementation for computing their own derivatives, this greatly speeds the computation of the local derivatives needed for learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "![](images/deep_learning_training.png)\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- AutoGrad is PyTorch's automatic differentiation engine.\n",
    "\n",
    "- We will see how to use AutoGrad to implement our own custom neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding Exercise 2.1: Buiding a Computational Graph\n",
    "\n",
    "- In PyTorch, to indicate that a certain tensor contains learnable parameters, we can set the optional argument `requires_grad` to `True`. \n",
    "\n",
    "- PyTorch will then track every operation using this tensor while configuring the computational graph. \n",
    "\n",
    "- For this exercise, use the provided tensors to build the following graph, which implements a single neuron with scalar input and output.\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><img src=\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W1D2_LinearDeepLearning/static/simple_graph.png\" alt=\"Simple nn graph\" width=\"600\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "![](/Users/shamsuddeenmuhammad/Documents/VScode/python4dataanalysis/images/autograd.png)\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial weight = -0.5, \n",
      "initial bias = 0.5\n",
      "for x=1 and y=7, prediction=0.0, and L2 Loss = 49.0\n"
     ]
    }
   ],
   "source": [
    "class SimpleGraph:\n",
    "  \"\"\"\n",
    "  Implementing Simple Computational Graph\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, w, b):\n",
    "    \"\"\"\n",
    "    Initializing the SimpleGraph\n",
    "\n",
    "    Args:\n",
    "      w: float\n",
    "        Initial value for weight\n",
    "      b: float\n",
    "        Initial value for bias\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    assert isinstance(w, float)\n",
    "    assert isinstance(b, float)\n",
    "    self.w = torch.tensor([w], requires_grad=True)\n",
    "    self.b = torch.tensor([b], requires_grad=True)\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"\n",
    "    Forward pass\n",
    "\n",
    "    Args:\n",
    "      x: torch.Tensor\n",
    "        1D tensor of features\n",
    "\n",
    "    Returns:\n",
    "      prediction: torch.Tensor\n",
    "        Model predictions\n",
    "    \"\"\"\n",
    "    assert isinstance(x, torch.Tensor)    \n",
    "    \n",
    "    #################################################\n",
    "    ## Implement the the forward pass to calculate prediction\n",
    "    ## Note that prediction is not the loss, but the value after `tanh`\n",
    "    # Complete the function and remove or comment the line below\n",
    "    raise NotImplementedError(\"Forward Pass `forward`\")\n",
    "    #################################################\n",
    "    prediction = torch.tanh() \n",
    "    return prediction\n",
    "\n",
    "\n",
    "def sq_loss(y_true, y_prediction):\n",
    "  \"\"\"\n",
    "  L2 loss function\n",
    "\n",
    "  Args:\n",
    "    y_true: torch.Tensor\n",
    "      1D tensor of target labels\n",
    "    y_prediction: torch.Tensor\n",
    "      1D tensor of predictions\n",
    "\n",
    "  Returns:\n",
    "    loss: torch.Tensor\n",
    "      L2-loss (squared error)\n",
    "  \"\"\"\n",
    "  assert isinstance(y_true, torch.Tensor)\n",
    "  assert isinstance(y_prediction, torch.Tensor)\n",
    "  #################################################\n",
    "  ## Implement the L2-loss (squred error) given true label and prediction\n",
    "  # Complete the function and remove or comment the line below\n",
    "  raise NotImplementedError(\"Loss function `sq_loss`\")\n",
    "  #################################################\n",
    "  loss = ...\n",
    "  return loss\n",
    "\n",
    "\n",
    "# Add event to airtable\n",
    "atform.add_event('Coding Exercise 2.1: Computational Graph ')\n",
    "\n",
    "feature = torch.tensor([1])  # Input tensor\n",
    "target = torch.tensor([7])  # Target tensor\n",
    "## Uncomment to run\n",
    "# simple_graph = SimpleGraph(-0.5, 0.5)\n",
    "# print(f\"initial weight = {simple_graph.w.item()}, \"\n",
    "#       f\"\\ninitial bias = {simple_graph.b.item()}\")\n",
    "# prediction = simple_graph.forward(feature)\n",
    "# square_loss = sq_loss(target, prediction)\n",
    "# print(f\"for x={feature.item()} and y={target.item()}, \"\n",
    "#       f\"prediction={prediction.item()}, and L2 Loss = {square_loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "895aa688263384567493505d09875376e7685297a6922210da729f70c5caa3cf"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('datascience')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
